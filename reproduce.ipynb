{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import load_feature_tensor_data, set_seed, load_tensor_dataset, load_feature_tensor_data_full, load_data\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from trainer import Trainer\n",
    "from torch import nn, optim\n",
    "from mlp import basic_mlp\n",
    "from pretrained_models import HubertClassifier, Wav2Vec2Classifier\n",
    "import torch\n",
    "from custom_transformer import AudioTransformer\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise class handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from noise_pp import split_noise\n",
    "split_noise('silence', 0.14,0.14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pretrained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 51386\n",
      "Validation samples: 6851\n",
      "Test samples: 6888\n",
      "Label distribution in training set:\n",
      "yes: 1860\n",
      "no: 1853\n",
      "up: 1843\n",
      "down: 1842\n",
      "left: 1839\n",
      "right: 1852\n",
      "on: 1864\n",
      "off: 1839\n",
      "stop: 1885\n",
      "go: 1861\n",
      "silence: 292\n",
      "unknown: 32556\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_feature_tensor_data(audio_dir=\"tensorflow-speech-recognition-challenge/train/audio\",\n",
    "    val_txt=\"tensorflow-speech-recognition-challenge/train/validation_list.txt\",\n",
    "    test_txt=\"tensorflow-speech-recognition-challenge/train/testing_list.txt\",\n",
    "    batch_size=16,\n",
    "    sample_rate=16000) # change to load_feature_tensor_data_full for RNN and LSTM\n",
    "\n",
    "weights = torch.tensor([0.06074972, 0.06097921, 0.06131008, 0.06134336, 0.06144344,\n",
    "       0.06101214, 0.06061936, 0.06144344, 0.05994402, 0.06071708,\n",
    "       6.38696739, 0.00347077]).to('cuda') # Change to appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME    = 'test_model'\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basic_mlp(12)\n",
    "model.name = MODEL_NAME\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n",
    "criterion = nn.CrossEntropyLoss(weight=weights) # change to None for unweighted\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Trener\n",
    "trainer = Trainer(\n",
    "    model_instance=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    test_loader=test_loader\n",
    ")\n",
    "\n",
    "# Trening\n",
    "trainer.train_multiple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = load_tensor_dataset('wav2vec_train.pt', batch_size=8)\n",
    "val_loader = load_tensor_dataset('wav2vec_valid.pt', batch_size=8)\n",
    "test_loader =load_tensor_dataset('wav2vec_test.pt', batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Custom transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = load_data(audio_dir=\"tensorflow-speech-recognition-challenge/train/audio\",\n",
    "    val_txt=\"tensorflow-speech-recognition-challenge/train/validation_list.txt\",\n",
    "    test_txt=\"tensorflow-speech-recognition-challenge/train/testing_list.txt\",\n",
    "    batch_size=16,\n",
    "    sample_rate=16000)\n",
    "\n",
    "weights = torch.tensor([0.06074972, 0.06097921, 0.06131008, 0.06134336, 0.06144344,\n",
    "       0.06101214, 0.06061936, 0.06144344, 0.05994402, 0.06071708,\n",
    "       6.38696739, 0.00347077]).to('cuda') # Change to appropriate device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME    = 'test_model'\n",
    "LEARNING_RATE = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AudioTransformer(num_classes=12, patch_time=8)\n",
    "model.name = MODEL_NAME\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "# Trener\n",
    "trainer = Trainer(\n",
    "    model_instance=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=val_loader,\n",
    "    test_loader=test_loader\n",
    ")\n",
    "\n",
    "# Trening\n",
    "trainer.train(n_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDERSAMPLING_FACTOR = 0.1\n",
    "\n",
    "dataset = train_loader.dataset \n",
    "labels = dataset.tensors[1].numpy()\n",
    "indices_class_11 = np.where(labels == 11)[0]\n",
    "indices_other = np.where(labels != 11)[0]\n",
    "undersampled_class_11 = np.random.choice(indices_class_11, size=int(UNDERSAMPLING_FACTOR * len(indices_class_11)), replace=False)\n",
    "final_indices = np.concatenate([indices_other, undersampled_class_11])\n",
    "np.random.shuffle(final_indices)\n",
    "undersampled_dataset = Subset(dataset, final_indices)\n",
    "\n",
    "undersampled_loader = DataLoader(undersampled_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "\n",
    "# Use the undersampled_loader as the train_loader replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest remains the same"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m129",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m129"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
